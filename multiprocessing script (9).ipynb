{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import ast\n",
    "import subprocess\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "import shlex\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "class log_wrapper:\n",
    "    def __init__(self, cmdline, tag):\n",
    "        self.cmdline = cmdline\n",
    "        self.tag = tag\n",
    "\n",
    "    def __enter__(self):\n",
    "        print(f\"Running: {self.cmdline}\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if exc_type:\n",
    "            print(f\"Error occurred in {self.tag}: {exc_value}\")\n",
    "        else:\n",
    "            print(f\"{self.tag} completed successfully.\")\n",
    "\n",
    "class SensitivityAnalysis():\n",
    "    '''\n",
    "    This function lets LAKE model runs in parallel\n",
    "    '''\n",
    "    def __init__(self,work_dir = \"model_output\",path_to_workdir = \"LAKE-LAKE3.0\"):\n",
    "        #change this to your location on your VMs\n",
    "        self.path_to_workdir = path_to_workdir\n",
    "        self.file_setup = os.path.join(self.path_to_workdir,\"setup/testlake_setup.dat\")\n",
    "        self.file_driver = os.path.join(self.path_to_workdir,\"setup/testlake_driver.dat\")\n",
    "        self.file_data = os.path.join(self.path_to_workdir,\"data/testlake.dat\")\n",
    "        self.number = 0\n",
    "        self.target_string = \"\"\n",
    "        self.file_list = []\n",
    "        self.target_values = []\n",
    "        self.dictionaries = []\n",
    "        self.list_of_modifies_files = []\n",
    "        #change this parameter depending on your username in google cloud\n",
    "        self.setup_folder = \"home/kgurbanov\"\n",
    "        self.work_dir = work_dir\n",
    "        \n",
    "\n",
    "    def create_directories_auto(self, number_directories):\n",
    "        '''\n",
    "        Creates ``number_directories`` of directories and copies required files into them\n",
    "        \n",
    "        number_directories : int\n",
    "        '''\n",
    "        for i in range(number_directories):\n",
    "            #change this to where you want to create all directories\n",
    "            new_directory = os.path.join(self.work_dir,f\"LAKE{i}\")\n",
    "            self.create_directory(new_directory)\n",
    "            self.copy_required_files(new_directory)\n",
    "\n",
    "    def copy_required_files(self, new_directory):\n",
    "        #Copy necessary files to the new directory change this to your location on \n",
    "        #your computer if you are running from root directory then just leave it like that\n",
    "        source_files = [\n",
    "            \"driver_file.dat\",\n",
    "            \"results\",\n",
    "            \"setup_file.dat\",\n",
    "            \"setup\",\n",
    "            \"launch\",\n",
    "            \"crproj\",\n",
    "            \"lake.out\",\n",
    "            \"data\"\n",
    "        ]\n",
    "\n",
    "        for source in source_files:\n",
    "            self.copy_to_directory(source, new_directory)\n",
    "\n",
    "    def copy_to_directory(self, source, destination_directory):\n",
    "        source_path = os.path.join(self.path_to_workdir, source) if self.path_to_workdir else source\n",
    "#         print(source_path)\n",
    "        destination_path = os.path.join(destination_directory, source)\n",
    "        if os.path.exists(destination_path):\n",
    "#             print(f\"Skipping copying {source} to {destination_path} as it already exists.\")\n",
    "            return\n",
    "\n",
    "        if os.path.isfile(source_path):\n",
    "            shutil.copy2(source_path, destination_path)\n",
    "        elif os.path.isdir(source_path):\n",
    "            shutil.copytree(source_path, destination_path)\n",
    "            \n",
    "            \n",
    "    def create_files(self):\n",
    "        self.number = len(self.list_of_modifies_files)\n",
    "        for i, (setup_file, driver_file) in enumerate(self.list_of_modifies_files):\n",
    "            setup_filename = f\"../setup/YKD{i}_setup.dat\"\n",
    "            driver_filename = f\"../setup/YKD{i}_driver.dat\"\n",
    "\n",
    "            self.write_dictionary_to_file(setup_file, setup_filename)\n",
    "            self.write_dictionary_to_file(driver_file, driver_filename)\n",
    "\n",
    "        return self.file_list\n",
    "    def _copy_files(self,args):\n",
    "        source_file,destination_file = args\n",
    "        with open(source_file, \"r\") as setup_file, open(destination_file, \"w\") as new_setup_file:\n",
    "            shutil.copyfileobj(setup_file, new_setup_file)\n",
    "    def create_files_single(self, source_directory, destination_directory,experiment_name):\n",
    "        if not os.path.exists(destination_directory):\n",
    "            os.makedirs(destination_directory)\n",
    "\n",
    "#         print(f\"Copying files from {source_directory} to {destination_directory}\")\n",
    "\n",
    "        for filename in os.listdir(source_directory):\n",
    "            source_path = os.path.join(source_directory, filename)\n",
    "            destination_path = os.path.join(destination_directory, filename)\n",
    "\n",
    "            if source_path == destination_path:\n",
    "#                 print(f\"Skipping copying {source_path} to {destination_path} as it's the same file.\")\n",
    "                continue\n",
    "\n",
    "#             print(f\"Copying {source_path} to {destination_path}\")\n",
    "\n",
    "            # If the item is a file, copy it to the destination directory\n",
    "            if os.path.isfile(source_path):\n",
    "                shutil.copy2(source_path, destination_path)\n",
    "\n",
    "            # If the item is a directory, recursively call create_files_single\n",
    "            elif os.path.isdir(source_path):\n",
    "                self.create_files_single(source_path, destination_directory,experiment_name)\n",
    "\n",
    "    def create_data_file(self):\n",
    "        try:\n",
    "            if not os.path.exists(\"data\"):\n",
    "                os.makedirs(\"data\")\n",
    "\n",
    "            for i, (setup_file, driver_file) in enumerate(self.list_of_modifies_files):\n",
    "                data_filename = os.path.join(\"data\", f\"YKD{i}.dat\")\n",
    "                self._copy_files((self.file_data, data_filename))\n",
    "\n",
    "            print(\"Data files created successfully.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error creating data files: {e}\")\n",
    "    def generate_and_write_files(self, num_files):\n",
    "        \"\"\"\n",
    "        Generate and write tuples of driver and setup files to the \"setup\" folder.\n",
    "\n",
    "        Args:\n",
    "        num_files (int): The number of tuples to generate.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(os.path.join(self.work_dir, self.setup_folder)):\n",
    "            os.makedirs(os.path.join(self.work_dir, self.setup_folder))\n",
    "\n",
    "\n",
    "        for i in range(num_files):\n",
    "            setup_folder = os.path.join(self.work_dir,f\"LAKE{i}/setup\")\n",
    "            data_folder = os.path.join(self.work_dir,f\"LAKE{i}/data\")\n",
    "            setup_filename = os.path.join(setup_folder, f\"YKD{i}_setup.dat\")\n",
    "            driver_filename = os.path.join(setup_folder, f\"YKD{i}_driver.dat\")\n",
    "            data_filename = os.path.join(setup_folder,f\"YKD{i}.dat\")\n",
    "\n",
    "            # Copy the content from the original setup and driver files to the new files\n",
    "            with open(self.file_setup, \"r\") as src_setup_file, open(setup_filename, \"w\") as dst_setup_file:\n",
    "                for line in src_setup_file:\n",
    "                    dst_setup_file.write(line.replace(\"\\t\", \" \"))  # Replace tabs with spaces\n",
    "\n",
    "            with open(self.file_driver, \"r\") as src_driver_file, open(driver_filename, \"w\") as dst_driver_file:\n",
    "                for line in src_driver_file:\n",
    "                    dst_driver_file.write(line.replace(\"\\t\", \" \"))  # Replace tabs with spaces\n",
    "            with open(self.file_data, \"r\") as src_driver_file, open(data_filename, \"w\") as dst_driver_file:\n",
    "                for line in src_driver_file:\n",
    "                    dst_driver_file.write(line.replace(\"\\t\", \" \"))  # Replace tabs with spaces\n",
    "            # Append the tuple of filenames to the list_of_modifies_files\n",
    "            self.list_of_modifies_files.append((setup_filename, driver_filename))\n",
    "\n",
    "\n",
    "    def find_target(self, targets, new_values,number):\n",
    "        \"\"\"\n",
    "        Search for the specified target string in the setup and driver files and change its value.\n",
    "\n",
    "        Args:\n",
    "            target (str): The string to search for in the setup and driver files.\n",
    "            new_values (list): A list of new values corresponding to each file in self.list_of_modifies_files.\n",
    "        \"\"\"\n",
    "        # Make a copy of the new_values list to avoid modifying the original list\n",
    "        \n",
    "        if number != len(self.list_of_modifies_files):\n",
    "            raise ValueError(f\"Number of new values({len(new_values)}) must be equal to the number of files in self.list_of_modifies_files({len(self.list_of_modifies_files)}).\")\n",
    "        for (setup_file, driver_file), target_value in zip(self.list_of_modifies_files, new_values):\n",
    "            for target,value in zip(targets,target_value):\n",
    "                if target == \"dataname\":\n",
    "                    new_value = new_values\n",
    "                    self.process_file_and_update_value(driver_file, target,target_value)\n",
    "                    continue\n",
    "\n",
    "                self.process_file_and_update_value(setup_file, target,value)\n",
    "\n",
    "                self.process_file_and_update_value(driver_file, target,value)\n",
    "\n",
    "    def process_file_and_update_value(self, file_path, target, new_value):\n",
    "        \"\"\"\n",
    "        Process the specified file, search for the target string, and update its value.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path of the file to process.\n",
    "            target (str): The string to search for in the file.\n",
    "            new_value (str): The new value to replace the found target string.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        with open(file_path, 'w') as file:\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "\n",
    "                if line.startswith(\"#\"):\n",
    "                    # Skip lines starting with #\n",
    "                    file.write(line + \"\\n\")\n",
    "                elif line.startswith(\"end\"):\n",
    "                    # Stop processing further lines after encountering \"end\"\n",
    "                    file.write(line + \"\\n\")\n",
    "                    break\n",
    "                elif target in line:\n",
    "                    file.write(f\"{target} {new_value}\\n\")\n",
    "                else:\n",
    "                    # Copy the line as is\n",
    "                    file.write(line + \"\\n\")\n",
    "\n",
    "    def read_file_path(self, filename):\n",
    "        # Read the file and return the content as a string\n",
    "        with open(filename, 'r') as file:\n",
    "            content = file.read().strip()\n",
    "        return content\n",
    "    @staticmethod\n",
    "    def create_directory(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        print(f\"Directory created: {directory}\")\n",
    "    def create_project_parallel(self, project_name,project_directory):\n",
    "    # Determine the operating system\n",
    "        \n",
    "        if os.name == \"posix\":\n",
    "            print(\"Operating system: Linux or macOS\")\n",
    "            OS = \"linux\" if os.uname().sysname == \"Linux\" else \"OSX\"\n",
    "        else:\n",
    "            print(\"Unknown operating system\")\n",
    "            return\n",
    "\n",
    "        directories = [\n",
    "            f\"{project_directory}/results/{project_name}/everystep\",\n",
    "            f\"{project_directory}/results/{project_name}/netcdf\",\n",
    "            f\"{project_directory}/results/{project_name}/time_series\",\n",
    "            f\"{project_directory}/results/{project_name}/hourly\",\n",
    "            f\"{project_directory}/results/{project_name}/monthly\",\n",
    "            f\"{project_directory}/results/{project_name}/daily\",\n",
    "        ]\n",
    "\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            pool.map(self.create_directory, directories)\n",
    "        \n",
    "        # Modify driver file\n",
    "        setup_folder = os.path.join(self.work_dir, project_directory)\n",
    "        driver_file_path = os.path.join(project_directory, \"driver_file.dat\")\n",
    "        print(\"Driver_file_path is\",driver_file_path)\n",
    "        if OS == \"linux\":\n",
    "            sed_command = f\"sed -i '2d' {driver_file_path} && sed -i \\\"\\\\$a setup/{project_name}_driver.dat\\\" {driver_file_path}\"\n",
    "        elif OS == \"OSX\":\n",
    "            sed_command = f\"sed -i '' '2d' {driver_file_path} && sed -i '' '$ a\\\\setup/{project_name}_driver.dat' {driver_file_path}\"\n",
    "        os.system(sed_command)\n",
    "\n",
    "        # Modify setup file\n",
    "        setup_file_path = os.path.join(project_directory, \"setup_file.dat\")\n",
    "        print(setup_file_path)\n",
    "        if OS == \"linux\":\n",
    "            sed_command = f\"sed -i '2d' {setup_file_path} && sed -i \\\"\\\\$a setup/{project_name}_setup.dat\\\" {setup_file_path}\"\n",
    "        elif OS == \"OSX\":\n",
    "            sed_command = f\"sed -i '' '2d' {setup_file_path} && sed -i '' '$ a\\\\setup/{project_name}_setup.dat' {setup_file_path}\"\n",
    "        os.system(sed_command)\n",
    "\n",
    "        # Check if necessary files exist\n",
    "        def check_file(file_path):\n",
    "            if not os.path.isfile(file_path):\n",
    "                print(f\"Warning: The file {file_path} does not exist\")\n",
    "\n",
    "        check_file(f\"./setup/{project_name}_setup.dat\")\n",
    "        check_file(f\"./setup/{project_name}_driver.dat\")\n",
    "        check_file(f\"./data/{project_name}.dat\")\n",
    "\n",
    "        print(\"Project for LAKE model created successfully.\")\n",
    "\n",
    "        \n",
    "    #create multiple projects\n",
    "    def create_multiple_projects(self,project_names,project_directories):\n",
    "        processes = []\n",
    "        for project_name,project_directory in zip(project_names,project_directories):\n",
    "            print(project_directory)\n",
    "            process = multiprocessing.Process(target=self.create_project_parallel, args=(project_name,project_directory))\n",
    "            process.daemon = False  # Set daemon to False to avoid the AssertionError\n",
    "            processes.append(process)\n",
    "\n",
    "        for process in processes:\n",
    "            process.start()\n",
    "\n",
    "        for process in processes:\n",
    "            process.join()\n",
    "\n",
    "        print(\"All projects created successfully.\")\n",
    "\n",
    "    def run_experiment(self, experiment_name,rundirectory,project_directory):\n",
    "        if experiment_name:\n",
    "            os.makedirs(f\"results/{experiment_name}\", exist_ok=True)\n",
    "\n",
    "        self.run_model(rundirectory,project_directory)\n",
    "        if experiment_name:\n",
    "            basename = experiment_name.rsplit(\"_\", 1)[0]\n",
    "            source_directory = os.path.join(f\"{project_directory}/results/{basename}\")\n",
    "#             Create a timestamp directories\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            destination_directory_with_timestamp = f\"results/{experiment_name}_{timestamp}\"\n",
    "\n",
    "            # Copy files from source_directory to the new directory with a timestamp\n",
    "            self.create_files_single(source_directory, destination_directory_with_timestamp, experiment_name)\n",
    "\n",
    "    def generate_samples_for_SA(self, p_name, p_initial, perturbation, logparams, N, seed=''):\n",
    "        params = []  # Dictionary of parameters\n",
    "\n",
    "        for name, init in zip(p_name, p_initial):\n",
    "            p_bounds = [init - (init * perturbation), init + (init * perturbation)]\n",
    "            params.append(dict(name=name, bounds=p_bounds, initial=init))\n",
    "\n",
    "        # Set random seed if provided\n",
    "        if seed != '':\n",
    "            np.random.seed(int(seed))\n",
    "\n",
    "        l = np.random.uniform(size=(N, len(params)))\n",
    "\n",
    "        # Generate bounds, based on specification in params list\n",
    "        lows = np.array([p['bounds'][0] for p in params])\n",
    "        highs = np.array([p['bounds'][1] for p in params])\n",
    "\n",
    "        # Figure out the spread, or difference between bounds\n",
    "        spreads = highs - lows\n",
    "\n",
    "        # Generate the sample matrix\n",
    "        sm = l * spreads + lows\n",
    "\n",
    "        # Apply loguniform for small param values only\n",
    "        if len(logparams) > 0:\n",
    "            inum = 0\n",
    "            for ilog, p in zip(logparams, params):\n",
    "                if ilog:\n",
    "                    sm[:, inum] = loguniform.rvs(p['bounds'][0], p['bounds'][1], size=N)\n",
    "                inum += 1\n",
    "\n",
    "        return sm\n",
    "\n",
    "    def run_experiment_parallel(self, experiment_names,rundirectory,project_directories):\n",
    "        processes = []\n",
    "        for experiment_name,project_directory in zip(experiment_names,project_directories):\n",
    "            process = multiprocessing.Process(target=self.run_experiment, args=(experiment_name, rundirectory,project_directory))\n",
    "            processes.append(process)\n",
    "\n",
    "        for process in processes:\n",
    "            process.start()\n",
    "\n",
    "        for process in processes:\n",
    "            process.join()\n",
    "\n",
    "        print(\"All experiments completed successfully.\")\n",
    "\n",
    "    def run_model(self,rundirectory,project_directory):\n",
    "        program_path = \"./lake.out\"\n",
    "        print(project_directory)\n",
    "        run_d = os.path.join(rundirectory,f\"{project_directory}\")\n",
    "        try:\n",
    "            completed_process = subprocess.run(\n",
    "                program_path,\n",
    "                shell=True,\n",
    "                check=True,\n",
    "                cwd=run_d,\n",
    "                stdout=subprocess.PIPE,  # Capture standard output\n",
    "                stderr=subprocess.PIPE,  # Capture standard error\n",
    "                text=True               # Return output as text\n",
    "            )\n",
    "\n",
    "            print(\"Model run completed successfully.\")\n",
    "            print(\"Standard Output:\")\n",
    "            print(completed_process.stdout)\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error occurred during model run. Exit code: {e.returncode}\")\n",
    "            print(\"Standard Output:\")\n",
    "            print(e.stdout)\n",
    "            print(\"Standard Error:\")\n",
    "            print(e.stderr)\n",
    "            raise e\n",
    "\n",
    "    def clear(self):\n",
    "        return self.list_of_modifies_files.clear()\n",
    "    \n",
    "    def create_and_run_sensitivity_analysis(self, number, p_name, target_values, rundirectory):\n",
    "        directories = []\n",
    "        project_names = []\n",
    "        for i in range(number):\n",
    "            directories.append(os.path.join(self.work_dir, f\"LAKE{i}\"))\n",
    "\n",
    "            project_names.append(f\"YKD{i}\")\n",
    "        print(directories)\n",
    "        print(project_names)\n",
    "        self.clear()\n",
    "#         self.create_data_file()\n",
    "\n",
    "        self.create_directories_auto(number)\n",
    "        self.generate_and_write_files(number)\n",
    "        self.find_target([\"dataname\"], project_names, number)\n",
    "        list_of_modified_files = self.find_target(p_name, target_values, number)\n",
    "        self.create_multiple_projects(project_names, directories)\n",
    "        self.run_experiment_parallel(project_names, rundirectory, directories)\n",
    "    def clear_workdir(self):\n",
    "        shutil.rmtree(self.work_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1030.513340057069, 594.526312899216, 1.3203296649669627e-07, 39606797292.77371], [1239.2654442790117, 327.7216013709123, 1.9185281052466014e-07, 9414046009.746094], [1620.2901024448129, 1031.7258943979143, 4.6880461317150614e-08, 63714025692.996254], [3143.262257130193, 187.51461562034527, 1.1449743476911626e-07, 59449081082.42545]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sensitivity = SensitivityAnalysis()\n",
    "p_name = ['khsO2', 'r0methprod',\"VmaxCH4aeroboxid\",\"khsCH4\"]\n",
    "p_initial = [2.1e+3,6.e+2,1.15e-7,3.75e+10]\n",
    "perturbation = 0.75\n",
    "logparams = np.zeros(len(p_initial))\n",
    "logparams[1] = 1\n",
    "N = 4\n",
    "seed = ''\n",
    "samples = sensitivity.generate_samples_for_SA(p_name, p_initial, perturbation, logparams, N, seed)\n",
    "target_values = samples.tolist()\n",
    "print(target_values)\n",
    "\n",
    "number = len(target_values)\n",
    "# number = sum(len(target_values[i]) for i in range(len(target_values)))\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_output/LAKE0', 'model_output/LAKE1', 'model_output/LAKE2', 'model_output/LAKE3']\n",
      "['YKD0', 'YKD1', 'YKD2', 'YKD3']\n",
      "Directory created: model_output/LAKE0\n",
      "Directory created: model_output/LAKE1\n",
      "Directory created: model_output/LAKE2\n",
      "Directory created: model_output/LAKE3\n",
      "model_output/LAKE0\n",
      "model_output/LAKE1\n",
      "model_output/LAKE2\n",
      "model_output/LAKE3\n",
      "Operating system: Linux or macOS\n",
      "Operating system: Linux or macOS\n",
      "Operating system: Linux or macOS\n",
      "Operating system: Linux or macOS\n",
      "Directory created: model_output/LAKE0/results/YKD0/netcdf\n",
      "Directory created: model_output/LAKE0/results/YKD0/everystep\n",
      "Directory created: model_output/LAKE0/results/YKD0/monthly\n",
      "Directory created: model_output/LAKE0/results/YKD0/hourly\n",
      "Directory created: model_output/LAKE1/results/YKD1/everystep\n",
      "Directory created: model_output/LAKE1/results/YKD1/netcdf\n",
      "Directory created: model_output/LAKE0/results/YKD0/time_series\n",
      "Directory created: model_output/LAKE0/results/YKD0/daily\n",
      "Directory created: model_output/LAKE1/results/YKD1/monthly\n",
      "Directory created: model_output/LAKE1/results/YKD1/daily\n",
      "Directory created: model_output/LAKE2/results/YKD2/netcdf\n",
      "Directory created: model_output/LAKE1/results/YKD1/hourly\n",
      "Directory created: model_output/LAKE2/results/YKD2/hourly\n",
      "Directory created: model_output/LAKE2/results/YKD2/monthly\n",
      "Directory created: model_output/LAKE1/results/YKD1/time_series\n",
      "Directory created: model_output/LAKE2/results/YKD2/everystep\n",
      "Directory created: model_output/LAKE2/results/YKD2/daily\n",
      "Directory created: model_output/LAKE2/results/YKD2/time_series\n",
      "Driver_file_path is model_output/LAKE0/driver_file.dat\n",
      "Driver_file_path is model_output/LAKE1/driver_file.dat\n",
      "Driver_file_path is model_output/LAKE2/driver_file.dat\n",
      "Directory created: model_output/LAKE3/results/YKD3/hourly\n",
      "Directory created: model_output/LAKE3/results/YKD3/time_series\n",
      "Directory created: model_output/LAKE3/results/YKD3/monthly\n",
      "model_output/LAKE0/setup_file.dat\n",
      "model_output/LAKE1/setup_file.dat\n",
      "Directory created: model_output/LAKE3/results/YKD3/daily\n",
      "model_output/LAKE2/setup_file.dat\n",
      "Project for LAKE model created successfully.\n",
      "Project for LAKE model created successfully.\n",
      "Directory created: model_output/LAKE3/results/YKD3/everystep\n",
      "Directory created: model_output/LAKE3/results/YKD3/netcdf\n",
      "Project for LAKE model created successfully.\n",
      "Driver_file_path is model_output/LAKE3/driver_file.dat\n",
      "model_output/LAKE3/setup_file.dat\n",
      "Project for LAKE model created successfully.\n",
      "All projects created successfully.\n",
      "model_output/LAKE0\n",
      "model_output/LAKE1\n",
      "model_output/LAKE2\n",
      "model_output/LAKE3\n"
     ]
    }
   ],
   "source": [
    "# sensitivity.clear()\n",
    "# sensitivity.create_directories_auto(number)\n",
    "# sensitivity.generate_and_write_files(number)\n",
    "# filename = [\"setup\",\"driver\"]\n",
    "# directory  = [\"LAKE0\",\"LAKE1\",\"LAKE2\",\"LAKE3\",\"LAKE4\"]\n",
    "# sensitivity.create_data_file()\n",
    "# sensitivity.find_target([\"dataname\"],[\"YKD0\",\"YKD1\",\"YKD2\",\"YKD3\",\"YKD4\"],number,[\"driver\",\"setup\"])\n",
    "# list_of_modified_files = sensitivity.find_target(p_name, target_values,number,filename)\n",
    "# folder = [\"YKD0\",\"YKD1\",\"YKD2\",\"YKD3\",\"YKD4\"]\n",
    "# # sensitivity.create_project_parallel(folder)\n",
    "# sensitivity.create_multiple_projects(folder,directory)\n",
    "# sensitivity.run_experiment_parallel(folder, rundirectory,directory)\n",
    "rundirectory = os.path.abspath(os.getcwd())\n",
    "# sensitivity.clear_workdir()\n",
    "sensitivity. create_and_run_sensitivity_analysis(number,p_name,target_values,rundirectory)\n",
    "# sensitivity.clear_workdir()\n",
    "#all results are going to be saved with in root results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
